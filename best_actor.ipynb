{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FER2013.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3m6Q4dHApSw",
        "colab_type": "text"
      },
      "source": [
        "# Best Actor - Mobile Game about Facial Expression based on Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opKtTDPQAtxN",
        "colab_type": "text"
      },
      "source": [
        "## Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-SQMHqaA9IF",
        "colab_type": "text"
      },
      "source": [
        "Best Actor is a mobile game which uses Convolutional Neural Network to detect facial expressions. Specifically, the mobile game will randomly displays a facial expression label (e.g. surprise) to the player. And the player has to mimic the facial expression so as to achieve a high score. This document will discuss the design and implementation details of the mobile game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYOEWCK1h_KP",
        "colab_type": "text"
      },
      "source": [
        "## Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHl6O0TFqRcw",
        "colab_type": "text"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4r-IHzgHr4E",
        "colab_type": "text"
      },
      "source": [
        "Considering that the mobile game relies on computer vision model to make predictions on human facial expressions, we need to download the dataset to train the computer vision model. Here we use FER2013 dataset in [Challenges in Representation Learning: Facial Expression Recognition Challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data) in Kaggle. Therefore, let's configure Kaggle API and download the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBNRIz_xoXGk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "fc56daa3-f6fa-419c-c8ff-d5a8ba677487"
      },
      "source": [
        "import os\n",
        "\n",
        "# Configure kaggle\n",
        "os.chdir('/root/')\n",
        "!mkdir -p .kaggle\n",
        "os.chdir('/root/.kaggle')\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y-o0TVcjehM8SZB3Nt8U3xkyeQu-Nse-' -O kaggle.json > /dev/null 2>&1\n",
        "!ls /root/.kaggle\n",
        "\n",
        "# Set permissions \n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Create data folder\n",
        "os.chdir('/content/')\n",
        "!rm -rf data\n",
        "!mkdir data\n",
        "os.chdir('data')\n",
        "!pwd\n",
        "\n",
        "# Download data\n",
        "!pip install -q kaggle\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "# Unzip data\n",
        "!unzip train.csv.zip train.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "/content/data\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content/data\n",
            " 84% 65.0M/77.3M [00:00<00:00, 55.5MB/s]\n",
            "100% 77.3M/77.3M [00:00<00:00, 99.3MB/s]\n",
            "Downloading icml_face_data.csv.zip to /content/data\n",
            " 80% 77.0M/96.6M [00:00<00:00, 55.6MB/s]\n",
            "100% 96.6M/96.6M [00:01<00:00, 99.6MB/s]\n",
            "Downloading test.csv.zip to /content/data\n",
            " 26% 5.00M/19.3M [00:00<00:00, 23.1MB/s]\n",
            "100% 19.3M/19.3M [00:00<00:00, 64.4MB/s]\n",
            "Downloading example_submission.csv to /content/data\n",
            "  0% 0.00/7.01k [00:00<?, ?B/s]\n",
            "100% 7.01k/7.01k [00:00<00:00, 6.71MB/s]\n",
            "Downloading fer2013.tar.gz to /content/data\n",
            " 91% 84.0M/92.0M [00:01<00:00, 66.4MB/s]\n",
            "100% 92.0M/92.0M [00:01<00:00, 90.5MB/s]\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlOpwuwwsEeG",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxhmhHcqJIqL",
        "colab_type": "text"
      },
      "source": [
        "The image dataset downloaded from Kaggle is in \".csv\" file format. Therefore, we need to load the \"train.csv\" file, and convert it to numpy array. The training images and labels are saved in \"x_train\" and \"y_train\" respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qvPVrA9sF7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "9a56231a-e447-4825-f330-43173726e122"
      },
      "source": [
        "import csv\n",
        "import numpy\n",
        "\n",
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "categories_count = {}\n",
        "\n",
        "with open('train.csv') as train:\n",
        "\n",
        "    # Read train.csv file\n",
        "    csv_reader = csv.reader(train)\n",
        "    next(csv_reader)  # Skip the header\n",
        "\n",
        "    for row in csv_reader:\n",
        "\n",
        "        # Append image\n",
        "        pixels_str = row[1]\n",
        "        pixels_list = [int(i) for i in pixels_str.split(' ')]\n",
        "        pixels_list = numpy.array(pixels_list, dtype='uint8')\n",
        "        image = pixels_list.reshape((48, 48))\n",
        "        train_images.append(image)\n",
        "\n",
        "        label_str = row[0]\n",
        "\n",
        "        # Calculate categories count\n",
        "        count = 0\n",
        "        if label_str in categories_count:\n",
        "            count = categories_count[label_str] + 1\n",
        "        categories_count[label_str] = count\n",
        "\n",
        "        # Append label\n",
        "        label = int(label_str)\n",
        "        train_labels.append(label)\n",
        "\n",
        "# Create numpy array of train images and labels\n",
        "x_train = numpy.array(train_images)\n",
        "y_train = numpy.array(train_labels)\n",
        "\n",
        "print('x_train shape: {0}'.format(x_train.shape))\n",
        "print('y_train shape: {0}'.format(y_train.shape))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4ae306d98809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcategories_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Read train.csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR8z57iCNgZN",
        "colab_type": "text"
      },
      "source": [
        "The image dataset provided by Kaggle contains 7 different facial expression categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The distogram of the facial expression categories is displayed as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYCX9KEPN3Rv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "a5475777-85f3-428c-a1c8-00569a1bda9e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "categories = ('Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral')\n",
        "y_pos = numpy.arange(len(categories))\n",
        "\n",
        "counts = []\n",
        "for label in range(len(categories)):\n",
        "    label_str = str(label)\n",
        "    count = categories_count[label_str]\n",
        "    counts.append(count)\n",
        "    plt.text(label - 0.25, count + 15, str(count))\n",
        "\n",
        "# Draw histogram\n",
        "plt.bar(y_pos, counts, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, categories)\n",
        "plt.ylabel('Count')\n",
        "plt.title('FER2013 Dataset Categories')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2e70537aea3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlabel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategories_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_str\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '0'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtWyvmDOvBB9",
        "colab_type": "text"
      },
      "source": [
        "Then, let's show one of the images in the dataset. Each image is grey-scale and contains 48 x 48 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAnAGc6bu8G_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = x_train[0]\n",
        "label = y_train[0]\n",
        "\n",
        "print('Label is: ' + str(label))\n",
        "plt.imshow(image, cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_7uKZdJuuku",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBEfQuN-KTZl",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to split the dataset into training set and test set. Here, we choose 20% of the dataset as test set, and the rest of the dataset as train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCly4_J8uwyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into train set and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train = x_train.reshape(len(x_train), 48, 48, 1)\n",
        "x_test = x_test.reshape(len(x_test), 48, 48, 1)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GgmBnVv_3ef",
        "colab_type": "text"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "194c87so_7ZK",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to train the model with the image dataset. Here we use Tensorlow as backend to train the model. Therefore, we need to import all the required packages before the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzFYmRS6GHCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "print('Tensorflow version: {}'.format(tf.__version__))\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Dropout, Flatten, Dense\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from tensorflow.keras import Model, Input\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhoJcURBGGT3",
        "colab_type": "text"
      },
      "source": [
        "By referring to VGGNet architecture, we have designed the computer vision model with several stacks of layers. The model will have the following components:\n",
        "- Convolutional layers: These layers are the building blocks of our architecture, which learns the image feature by computing the dot product between the weights and the small region on the image. Similar to VGGNet architecture, all the convolutional layers are designed with 3 x 3 kernal size, and several filters.\n",
        "- Activation functions: The activation functions are those functions which are applied to the outputs of the layers in the network. Specifically, we use ReLU (Rectified Linear Unit) activation function to increase the non-linearity of the network. Besides, a Softmax function will be used to compute the probability of each category.\n",
        "- Pooling layers: These layers will down-sample the image to reduce the spatial data and extract features. In our model, we will use Max Pooling with A 3 x 3 pooling size and a 2 x 2 stride.\n",
        "- Dense layers: The dense layers are stacked as the fully connected layers of the network, which take in the feature data from the previous convolutional layers and perform decision making.\n",
        "- Dropout layers: The dropout layers are used to prevent over-fitting when training the model.\n",
        "- Batch normalization: This technique can be used to speed up learning by normalizing the output of the previous activation layer. \n",
        "\n",
        "The diagram of the model is displayed as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kyS0TKDAbRU",
        "colab_type": "text"
      },
      "source": [
        "![cnn](https://drive.google.com/uc?id=1jjORxRgvEDDMLZ-mX5JkCnUWUFn7IHpL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7hcc_NfLpII",
        "colab_type": "text"
      },
      "source": [
        "Our model contains 5 stacks of layers. In each of the first 4 stacks of layers, there are 2 convolutional layer followed by 1 pooling layer. Besides, we use batch normalization to speed up training and dropout to prevent over-fitting. Then we have one stack of 3 fully-connected layers, followed by a Softmax activation function, which generates the probability of the facial expression categories. Finally, we compile our model using Adam optimizer with a certain learning rate. Considering that we are dealing with classification issue, we will use `sparse_categorical_crossentropy` as the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSFEgkm1BntS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model = Sequential()\n",
        "\n",
        "# 1st convolution layer\n",
        "cnn_model.add(Conv2D(64, input_shape=(48, 48, 1), kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(64, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "# 2nd convolution layer\n",
        "cnn_model.add(Conv2D(128, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(128, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "# 3rd convolution layer\n",
        "cnn_model.add(Conv2D(256, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(256, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "# 4th convolution layer\n",
        "cnn_model.add(Conv2D(512, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(Conv2D(512, padding='same', kernel_size=(3, 3), activation='relu'))\n",
        "cnn_model.add(BatchNormalization())\n",
        "cnn_model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "# Fully connected layer\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(512, activation='relu'))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "cnn_model.add(Dense(256, activation='relu'))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "cnn_model.add(Dense(64, activation='relu'))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "cnn_model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "adam = Adam(learning_rate=0.001)\n",
        "cnn_model.compile(optimizer=adam,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Summary the model\n",
        "cnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAp0dm5pWxfZ",
        "colab_type": "text"
      },
      "source": [
        "Next, we will train our model. Here we use Early Stopping strategy, which will stop the training process when there is no improvement in the validation accuracy. Besides, we will also reduce the learning rate by a specific factor if there is a plateau is detected in the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyXIM0otH6yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=5)\n",
        "\n",
        "# Train the model\n",
        "history = cnn_model.fit(x_train,\n",
        "                        y_train,\n",
        "                        batch_size=64,\n",
        "                        epochs=100,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        callbacks=[early_stopping, reduce_learning_rate])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZcGlpb4YkCs",
        "colab_type": "text"
      },
      "source": [
        "After the training process is completed, let's display the accuracy diagram of the training accuracy and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24B7OVNENAHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show accuracy diagram\n",
        "plt.title('Model Accuracy')\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Accuracy', 'Validation Accuracy'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOdICivdd8EG",
        "colab_type": "text"
      },
      "source": [
        "### Improve Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htDtEoQCah-2",
        "colab_type": "text"
      },
      "source": [
        "We will try to improve our model to increase the validation accuracy. Here we have used the following techniques in our model improvement:\n",
        "- Generate hard data\n",
        "- Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dZxCnGWbcog",
        "colab_type": "text"
      },
      "source": [
        "Before we improve our model, let's evaluate our model by using test set. We can see the score before improvement is around 65%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLne6n_fd8gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the model before improvement\n",
        "_, score_before_improvement = cnn_model.evaluate(x_test, y_test)\n",
        "print('Score before improvement: {}'.format(score_before_improvement))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAyZUhqZb7RP",
        "colab_type": "text"
      },
      "source": [
        "Next, we will improve our model by generating hard data. Specifically, instead of training the model over and over again, we will select the images which are incorrectly labelled by the model, and train the model on these specific images. Therefore, let's use our model to make predictions first, and put the incorrect ones into the array of hard data for further training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlfx3y7deSTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate hard data\n",
        "hard_images = []\n",
        "hard_labels = []\n",
        "\n",
        "# Make predictions\n",
        "predictions = cnn_model.predict(x_test)\n",
        "for i, v in enumerate(predictions):\n",
        "    y_predict = numpy.argmax(v)\n",
        "    y_real = y_train[i]\n",
        "    if y_predict != y_real:\n",
        "        # If predict incorrectly, append to array\n",
        "        image = x_train[i]\n",
        "        hard_image = image.reshape(1, 48, 48, 1)\n",
        "        hard_images.append(image)\n",
        "        hard_labels.append(y_real)\n",
        "\n",
        "x_hard = numpy.array(hard_images)\n",
        "y_hard = numpy.array(hard_labels)\n",
        "\n",
        "print(x_hard.shape)\n",
        "print(y_hard.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL5YlYNbdXGw",
        "colab_type": "text"
      },
      "source": [
        "Next, we will train our model on these specific images which are previously incorrectly labelled by our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHlPxTfryF1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model on hard data\n",
        "x_hard_train, x_hard_test, y_hard_train, y_hard_test = train_test_split(x_hard, y_hard, test_size=0.2)\n",
        "history = cnn_model.fit(x_hard_train,\n",
        "                        y_hard_train,\n",
        "                        batch_size=64,\n",
        "                        epochs=100,\n",
        "                        validation_data=(x_hard_test, y_hard_test),\n",
        "                        callbacks=[early_stopping, reduce_learning_rate])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDoZIFuMdl5H",
        "colab_type": "text"
      },
      "source": [
        "Of course, this might cause the over-fitting issue on these incorrectly labelled images. Therefore, we will train the model again to balance out. Besides, we will also perform data augmentation to diversify our training dataset by rotating, shifting, zooming or flipping the images, which will also improve our model to overcome the over-fitting issue and learn the generic features of each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1a3Gknqyl9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform data augmentation\n",
        "data_generator = ImageDataGenerator(featurewise_center=False,\n",
        "                                    featurewise_std_normalization=False,\n",
        "                                    rotation_range=10,\n",
        "                                    width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1,\n",
        "                                    zoom_range=.1,\n",
        "                                    horizontal_flip=True)\n",
        "flow = data_generator.flow(x_train, \n",
        "                           y_train, \n",
        "                           batch_size=64)\n",
        "\n",
        "# Train the model again to balance out\n",
        "history = cnn_model.fit(flow,\n",
        "                        epochs=100,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        callbacks=[early_stopping, reduce_learning_rate])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woUbYgY3fVey",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's evaluate our model again after improvement. Previously, we have the validation accuracy at around 65%. Nevertheless, the validation accuracy has improved to around 67%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsSyAzy4e2_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the model after improvement\n",
        "_, score_after_improvement = cnn_model.evaluate(x_test, y_test)\n",
        "print('Score after improvement: {}'.format(score_after_improvement))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lH6xHC8bj0K",
        "colab_type": "text"
      },
      "source": [
        "If we compare with the Kaggle competition, our score is around 67% and can be ranked within the top 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGqXqUt8ZwCn",
        "colab_type": "text"
      },
      "source": [
        "![kaggle](https://drive.google.com/uc?id=1UbeKmOjh3_3NnSXOmmNMclAQeiqnkCeq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BhmDuNVD6gz",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq3q_MMdZupg",
        "colab_type": "text"
      },
      "source": [
        "Next, we will make predictions and create the confusion matrix. Since FER2013 dataset does NOT provide too many images labelled with \"Disgust\", we can tell from the confusion matrix that, the model might not be able to classify the images labelled with \"Disgust\" very well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZBETwWhckX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U mlxtend > /dev/null 2>&1\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Make predictions\n",
        "predictions = cnn_model.predict(x_test)\n",
        "y_predict = numpy.argmax(predictions, axis=1)\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_predict)\n",
        "\n",
        "# Display confusion matrix\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "fig, ax = plot_confusion_matrix(conf_mat=conf_matrix, class_names=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijiykST1LHzh",
        "colab_type": "text"
      },
      "source": [
        "Then, let's try some other human facial expression images found online.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8HVZFf8Lmun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download into data folder\n",
        "os.chdir('/content/data')\n",
        "\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ommGLsYSnmX8846iyQigKQfIfvpQzsF0' -O happy.jpg > /dev/null 2>&1\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1I1IjDm9Az4pkDqOz3zcQAguEJ1OsWagt' -O sad.jpg > /dev/null 2>&1\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ZLglz_y0QklbYTcwJW1wH0YnAwBn6-1I' -O surprise.jpg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtII1ObCLRV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "def predict_facial_expression(image_path):\n",
        "    \"\"\"Predict facial expression for image\"\"\"\n",
        "    # Open image\n",
        "    rgb_image = image.load_img(path=image_path,\n",
        "                            color_mode='grayscale',\n",
        "                            target_size=(48, 48))\n",
        "    # Convert to array\n",
        "    rgb_array = image.img_to_array(rgb_image)\n",
        "    rgb_array = numpy.expand_dims(rgb_array, axis=0)\n",
        "\n",
        "    # Show image\n",
        "    rgb_display = numpy.array(rgb_array, 'float32')\n",
        "    rgb_display = rgb_display.reshape([48, 48]);\n",
        "    plt.imshow(rgb_display, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    # Make predictions\n",
        "    rgb_array = rgb_array / 255.0\n",
        "    prediction = cnn_model.predict(rgb_array)\n",
        "\n",
        "    # Draw the histogram\n",
        "    plt.bar(y_pos, prediction[0], align='center', alpha=0.5)\n",
        "    plt.xticks(y_pos, categories)\n",
        "    plt.ylabel('Percentage')\n",
        "    plt.title('Facial Expression Prediction')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68HuO6eOXsTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Happy\n",
        "predict_facial_expression('happy.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X32JxVqxZwRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sad\n",
        "predict_facial_expression('sad.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SHHG9NiZzVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Surprise\n",
        "predict_facial_expression('surprise.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QXPUCa2bpVa",
        "colab_type": "text"
      },
      "source": [
        "### Export Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhTz94mYgenS",
        "colab_type": "text"
      },
      "source": [
        "Last but not least, let's export the model, and save our TensorFlow and TensorFlow Lite model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAeGSuT4J9fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create build folder\n",
        "os.chdir('/content/')\n",
        "!rm -rf build\n",
        "!mkdir build\n",
        "os.chdir('build')\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfQNCPLtEIYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save tensorflow model\n",
        "cnn_model.save('FER2013.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFGQoUgnFtwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save tensorflow lite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(cnn_model)\n",
        "tflite_model = converter.convert()\n",
        "open(\"FER2013.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl8zZhYxKYE9",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z2aRqpmKZr_",
        "colab_type": "text"
      },
      "source": [
        "To put it in a nutshell, Best Actor is a mobile game which integrates the computer vision technology using Convolutional Neural Network."
      ]
    }
  ]
}